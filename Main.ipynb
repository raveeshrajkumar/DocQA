{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ebf948",
   "metadata": {},
   "source": [
    "# Document Based Question Answering System\n",
    "* Base Research Paper: https://arxiv.org/pdf/1805.08092.pdf\n",
    "* Other References:\n",
    "    * https://ieeexplore.ieee.org/abstract/document/9079274\n",
    "    * https://arxiv.org/pdf/1707.07328.pdf\n",
    "    * https://arxiv.org/pdf/1810.04805.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bae9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ecc30",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc02e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import pdfplumber\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92fb6410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/akshay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import pprint\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import Word2Vec  \n",
    "import gensim.downloader as api  \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0164196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db978b",
   "metadata": {},
   "source": [
    "# Impoting Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3836ccd",
   "metadata": {},
   "source": [
    "# Extracting text from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acfc8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(pdf_path):\n",
    "    \"\"\"\n",
    "    Function to extract text from a PDF file.\n",
    "\n",
    "    Parameters:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9efb012",
   "metadata": {},
   "source": [
    "# Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3286caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Function to clean the text by removing special characters, stopwords, and other irrelevant information.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    str: Cleaned text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove abnormal characters\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        \n",
    "    # Remove authors' names and specific dataset names\n",
    "    cleaned_text = re.sub(r'\\b[A-Z]+\\s[A-Z]\\s[A-Z]+(\\s-\\s[A-Z]\\s-\\s\\d+)\\b', '', text)\n",
    "\n",
    "    # Remove section headings\n",
    "    cleaned_text = re.sub(r'\\b\\d+\\.\\s[A-Z]+\\b', '', cleaned_text)\n",
    "\n",
    "    cleaned_text = re.sub(r\" â€¢ \", \" \", text)  # Replace bullet point with a single space\n",
    " \n",
    "    # Remove extra whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "\n",
    "    # Remove stopwords using NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_text = ' '.join(word for word in cleaned_text.split() if word.lower() not in stop_words)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d80b970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence, stopwords=False):\n",
    "    sentence = sentence.lower().strip() # Convert the sentence to lowercase and remove leading/trailing whitespaces\n",
    "    sentence = re.sub(r'[^a-z0-9\\s]', '', sentence) # Remove any characters that are not alphabets, digits, or whitespaces\n",
    "    if stopwords:\n",
    "        sentence = remove_stopwords(sentence) # Optionally remove stopwords from the sentence\n",
    "    return sentence\n",
    "\n",
    "def get_cleaned_sentences(tokens, stopwords=False):\n",
    "    cleaned_sentences = [] # Initialize an empty list to store cleaned sentences\n",
    "    for row in tokens: # Iterate over each row in the tokens\n",
    "        cleaned = clean_sentence(row, stopwords) # Clean the sentence using the clean_sentence function\n",
    "        cleaned_sentences.append(cleaned) # Append the cleaned sentence to the list of cleaned_sentences\n",
    "    return cleaned_sentences # Return the list of cleaned sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ccdf78",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8546b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text):\n",
    "    \"\"\"\n",
    "    Function to chunk text into smaller segments for processing.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text to be chunked.\n",
    "\n",
    "    Returns:\n",
    "    list: List of text chunks.\n",
    "    \"\"\"\n",
    "    splitter = TokenTextSplitter(\n",
    "        encoding_name=\"gpt2\",\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "    output = splitter.create_documents([text])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42c29b",
   "metadata": {},
   "source": [
    "## Model based on Bag Of Words and Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905499ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveAndPrintFAQAnswer(question_embedding, sentence_embeddings, sentences):\n",
    "    max_sim = -1  # Initialize the maximum similarity score\n",
    "    index_sim = -1  # Initialize the index of the most similar sentence\n",
    "    for index, embedding in enumerate(sentence_embeddings):  # Iterate over the sentence embeddings\n",
    "        # Compute the cosine similarity between the question embedding and the current sentence embedding\n",
    "        sim = cosine_similarity(embedding, question_embedding)[0][0]\n",
    "        if sim > max_sim:  # If the current similarity is greater than the maximum similarity found so far\n",
    "            max_sim = sim  # Update the maximum similarity\n",
    "            index_sim = index  # Update the index of the most similar sentence\n",
    "  \n",
    "    return index_sim  # Return the index of the most similar sentence\n",
    "\n",
    "\n",
    "def naive_drive(pdf_text, question): \n",
    "    tokens = nltk.sent_tokenize(pdf_text)  # Tokenize the text into sentences\n",
    "    cleaned_sentences = get_cleaned_sentences(tokens, stopwords=True)  # Clean and preprocess the sentences\n",
    "    cleaned_sentences_with_stopwords = get_cleaned_sentences(tokens, stopwords=False)  # Clean sentences without removing stopwords\n",
    "    sentences = cleaned_sentences_with_stopwords  # Assign cleaned sentences to 'sentences'\n",
    "    sentence_words = [[word for word in document.split()] for document in sentences]  # Tokenize each sentence into words\n",
    "\n",
    "    dictionary = corpora.Dictionary(sentence_words)  # Create a dictionary from the tokenized words\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in sentence_words]  # Convert tokenized words into Bag-of-Words representation\n",
    "\n",
    "    question = clean_sentence(question, stopwords=False)  # Clean the question\n",
    "    question_embedding = dictionary.doc2bow(question.split())  # Convert the question into a Bag-of-Words representation\n",
    "\n",
    "    index = retrieveAndPrintFAQAnswer(question_embedding, bow_corpus, sentences)  # Retrieve the index of the most similar sentence\n",
    "    \n",
    "    return sentences[index]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376b576",
   "metadata": {},
   "source": [
    "## Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd04da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model successfully loaded\n"
     ]
    }
   ],
   "source": [
    "v2w_model = None  # Initializing Word2Vec model variable\n",
    "\n",
    "try:\n",
    "    v2w_model = gensim.models.KeyedVectors.load('./w2vecmodel.mod')  # Try to load the Word2Vec model from a local file\n",
    "    print(\"Word2Vec model successfully loaded\")  \n",
    "except FileNotFoundError:  # Handle the case when the local file is not found\n",
    "    v2w_model = api.load('word2vec-google-news-300')  # Load the pre-trained \"word2vec-google-news-300\" model from gensim downloader\n",
    "    v2w_model.save(\"./w2vecmodel.mod\")  # Save the loaded model to a local file for future use\n",
    "    print(\"Word2Vec model saved\")  \n",
    "\n",
    "w2vec_embedding_size = len(v2w_model['pc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9189a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordVec(word, model):\n",
    "    samp = model['pc']\n",
    "    vec = [0]*len(samp)\n",
    "    try:\n",
    "        vec = model[word]\n",
    "    except:\n",
    "        vec = [0]*len(samp)\n",
    "    return (vec)\n",
    "\n",
    "\n",
    "def getPhraseEmbedding(phrase, embeddingmodel):\n",
    "    samp = getWordVec('computer', embeddingmodel)\n",
    "    vec = np.array([0]*len(samp))\n",
    "    den = 0;\n",
    "    for word in phrase.split():\n",
    "        den = den+1\n",
    "        vec = vec + np.array(getWordVec(word, embeddingmodel))\n",
    "    return vec.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b91cdce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_drive(pdf_text, question):\n",
    "\n",
    "    tokens = nltk.sent_tokenize(pdf_text)  # Tokenize the text into sentences\n",
    "    cleaned_sentences_with_stopwords = get_cleaned_sentences(tokens, stopwords=False)  # Clean sentences without removing stopwords\n",
    "    sentences = cleaned_sentences_with_stopwords  # Assign cleaned sentences to 'sentences'\n",
    "    sentence_words = [[word for word in document.split()] for document in sentences]  # Tokenize each sentence into words\n",
    "\n",
    "    sent_embeddings = []  # Initialize a list to store embeddings of sentences\n",
    "    for sent in sentences:  # Iterate over each sentence\n",
    "        sent_embeddings.append(getPhraseEmbedding(sent, v2w_model))  # Generate the embedding for the sentence and append it to the list\n",
    "\n",
    "    question_embedding = getPhraseEmbedding(question, v2w_model)  # Generate the embedding for the question\n",
    "    index = retrieveAndPrintFAQAnswer(question_embedding, sent_embeddings, cleaned_sentences_with_stopwords)  # Retrieve the index of the most similar sentence\n",
    "    return cleaned_sentences_with_stopwords[index]  # Return the most similar sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065dfc21",
   "metadata": {},
   "source": [
    "## Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abb92197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Model Saved\n"
     ]
    }
   ],
   "source": [
    "glove_model = None\n",
    "try:\n",
    "    glove_model = gensim.models.Keyedvectors.load('./glovemodel.mod')\n",
    "    print(\"Glove Model Successfully loaded\")\n",
    "except:\n",
    "    glove_model = api.load('glove-twitter-25')\n",
    "    glove_model.save(\"./glovemodel.mod\")\n",
    "    print(\"Glove Model Saved\")\n",
    "\n",
    "glove_embedding_size = len(glove_model['pc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b837e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_drive(pdf_text, question):\n",
    "\n",
    "    tokens = nltk.sent_tokenize(pdf_text)\n",
    "    cleaned_sentences = get_cleaned_sentences(tokens, stopwords=True)\n",
    "    cleaned_sentences_with_stopwords = get_cleaned_sentences(tokens, stopwords=False)\n",
    "    sentences = cleaned_sentences_with_stopwords\n",
    "    sentence_words = [[word for word in document.split()] for document in sentences]\n",
    "\n",
    "    sent_embeddings = []\n",
    "    for sent in cleaned_sentences:\n",
    "        sent_embeddings.append(getPhraseEmbedding(sent, glove_model))\n",
    "\n",
    "    question_embedding = getPhraseEmbedding(question, glove_model)\n",
    "    index = retrieveAndPrintFAQAnswer(question_embedding, sent_embeddings, cleaned_sentences_with_stopwords)\n",
    "    return cleaned_sentences_with_stopwords[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f588ce",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc44e94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "bert_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "from transformers import BertTokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b43c8a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_bert(question, answer_text):\n",
    "    \"\"\"\n",
    "    Function to answer a given question based on the provided text using BERT model.\n",
    "\n",
    "    Parameters:\n",
    "    question (str): Question to be answered.\n",
    "    answer_text (str): Text containing the answer.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Answer, score, start scores, end scores, tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = bert_tokenizer.encode(question, answer_text, max_length=512, truncation=True)\n",
    "    sep_index = input_ids.index(bert_tokenizer.sep_token_id)\n",
    "\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    start_scores, end_scores = bert_model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids])).values()\n",
    "\n",
    "    all_tokens = bert_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    #print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n",
    "    #print(f'score: {torch.max(start_scores)}')\n",
    "    score = float(torch.max(start_scores))\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "    tokens = bert_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "        #if tokens[i][0:2] == ' ':\n",
    "         #   answer += tokens[i][2:]\n",
    "\n",
    "        #else:\n",
    "           # answer += ' ' + tokens[i]\n",
    "    return answer, score, start_scores, end_scores, tokens\n",
    "    #print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba68bf6",
   "metadata": {},
   "source": [
    "# Sentence Tokenize BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20325f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_split_sentences(pdf_text,max_tokens=256):\n",
    "    tokenized_text = []\n",
    "    sentences = nltk.sent_tokenize(pdf_text)\n",
    "    for sentence in sentences:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tokenized_text.extend(tokens)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_tokens = 0\n",
    "    for token in tokenized_text:\n",
    "        current_chunk.append(token)\n",
    "        current_chunk_tokens += 1\n",
    "        if current_chunk_tokens >= max_tokens:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_chunk_tokens = 0\n",
    "    if current_chunk_tokens > 0:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3916a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_drive_sent(text, question):\n",
    "\n",
    "    max_score = 0\n",
    "    final_answer = \"\"\n",
    "    new_df = expand_split_sentences(text)\n",
    "    tokens = []\n",
    "    s_scores = np.array([])\n",
    "    e_scores = np.array([])\n",
    "    \n",
    "    # Iterate over split sentences and find the best answer\n",
    "    for new_context in new_df:\n",
    "        ans, score, start_score, end_score, token = answer_question_bert(question, new_context)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            s_scores = start_score.detach().numpy().flatten()\n",
    "            e_scores = end_score.detach().numpy().flatten()\n",
    "            tokens = token\n",
    "            final_answer = ans\n",
    "    \n",
    "    #return new_df\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3354636e",
   "metadata": {},
   "source": [
    "# Chunking BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39d6b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking BERT Model\n",
    "\n",
    "def bert_drive_chunk(chunk, question):\n",
    "    \"\"\"\n",
    "    Function to drive the BERT model for answering a question.\n",
    "\n",
    "    Parameters:\n",
    "    chunk (list): List of text chunks.\n",
    "    question (str): Question to be answered.\n",
    "\n",
    "    Returns:\n",
    "    str: Final answer.\n",
    "    \"\"\"\n",
    "    text_chunks = [doc.page_content for doc in chunk]\n",
    "    \n",
    "    # Initialize variables\n",
    "    max_score = 0\n",
    "    final_answer = \"\"\n",
    "    tokens = []\n",
    "    s_scores = np.array([])\n",
    "    e_scores = np.array([])\n",
    "    \n",
    "    # Iterate over text chunks\n",
    "    for chunk in text_chunks:\n",
    "        ans, score, start_score, end_score, token = answer_question_bert(question, chunk)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            s_scores = start_score.detach().numpy().flatten()\n",
    "            e_scores = end_score.detach().numpy().flatten()\n",
    "            tokens = token\n",
    "            final_answer = ans\n",
    "    \n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb51c9e",
   "metadata": {},
   "source": [
    "# Flan T5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "431cd457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "flan_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "flan_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d423e996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flan_t5_drive(chunks, question):\n",
    "    \"\"\"\n",
    "    Function to drive the Flan-T5 model for answering a question.\n",
    "\n",
    "    Parameters:\n",
    "    chunks (list): List of text chunks.\n",
    "    question (str): Question to be answered.\n",
    "\n",
    "    Returns:\n",
    "    str: Final answer.\n",
    "    \"\"\"\n",
    "    input_text = \"Question: \" + question + \" Context: \"\n",
    "    max_score = 0\n",
    "    final_answer = \"\"\n",
    "    \n",
    "    # Iterate over text chunks\n",
    "    for chunk in chunks:\n",
    "        input_text_chunk = input_text + chunk.page_content\n",
    "        input_ids = flan_tokenizer.encode(input_text_chunk, return_tensors=\"pt\")\n",
    "        output_ids = flan_model.generate(input_ids, max_length=512, num_beams=4, early_stopping=True)\n",
    "        output_text = flan_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Implement a scoring mechanism to choose the best answer\n",
    "        # For example, you could use the length of the answer as a proxy for confidence\n",
    "        score = len(output_text)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            final_answer = output_text\n",
    "    \n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea11184a",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f275b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/akshay/Documents/Akshay_Resume.pdf'\n",
    "question = \"Which University did he study?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17aca8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = extract_pdf_text(file_path)\n",
    "cleaned_text = clean_text(pdf_text)\n",
    "chunks = chunk_text(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f314a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag Of Words :   collaborated automatic leaderboard system seamlessly updates ball enhancing user engagement\n"
     ]
    }
   ],
   "source": [
    "answer = naive_drive(cleaned_text, question)\n",
    "print(\"Bag Of Words : \",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c825bb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec :  akshay g  sgakshay10   akshay g   akshay21110189snuchennaieduin  h 91 9176090049 education shiv nadar university chennai sep 2021  may 2025 btech artificial intelligence data science cgpa 84100 class 12th central board secondary education 95 june\n"
     ]
    }
   ],
   "source": [
    "answer = word2vec_drive(cleaned_text, question)\n",
    "print(\"Word2Vec : \",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e34019cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Embeddings :   utilized bert model hugging faces transformers library understand abusive language context improving detection performance\n"
     ]
    }
   ],
   "source": [
    "answer = glove_drive(cleaned_text, question)\n",
    "print(\"Glove Embeddings : \",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7fa89af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization :  shiv nadar university\n"
     ]
    }
   ],
   "source": [
    "sent = bert_drive_sent(cleaned_text,question)\n",
    "print(\"Sentence Tokenization : \",sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f222b0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking :  shiv nadar university\n"
     ]
    }
   ],
   "source": [
    "chunk = bert_drive_chunk(chunks, question)\n",
    "print(\"Chunking : \",chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "409eca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flan-T5 Answer: Shiv Nadar University\n"
     ]
    }
   ],
   "source": [
    "flan_t5_answer = flan_t5_drive(chunks, question)\n",
    "print(\"Flan-T5 Answer:\", flan_t5_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9434ce4",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "#### Similarities:\n",
    "\n",
    "* The Bag of Words, Word2Vec, GloVe Embeddings, and Sentence Tokenization models seem to provide relevant snippets or sentences from the given text as their outputs.\n",
    "* The Chunking model also extracts a relevant snippet related to image segmentation and license plate isolation.\n",
    "\n",
    "#### Differences:\n",
    "\n",
    "* The Bag of Words model provides a more literal answer by extracting the sentence \"implemented image segmentation algorithms isolate license plates various backgrounds\" from the text, which directly answers the question about image segmentation projects.\n",
    "* The Word2Vec and GloVe Embeddings models provide the same output, which is not directly related to image segmentation projects. Instead, it talks about utilizing the BERT model for understanding abusive language and improving detection performance, which seems irrelevant to the given question.\n",
    "* The Sentence Tokenization model extracts the same sentence as the Bag of Words model, but it doesn't provide any additional context or information.\n",
    "* The Chunking model also extracts the relevant sentence about isolating license plates and implementing image segmentation algorithms, similar to the Bag of Words model.\n",
    "* The Flan-T5 model provides a more comprehensive answer, combining relevant information from the text. It mentions \"Developed Automatic Number Plate Recognition (ANPR) application license plate extraction character recognition\" and \"Implemented image segmentation algorithms isolate license plates various backgrounds.\" This answer seems to be the most relevant and informative for the given question.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
